{"cells":[{"cell_type":"markdown","metadata":{"id":"FWrDSSi8_nOS"},"source":["We are using BERT to identify and segregate hate speech comments from across the arrays of comments inside YouTube. for which we are using the google built in methods to fetch the required files and comments taken live from the YouTube, comments are fetched on the network of INDIA to define the same."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"31vo6H0p_Pus","executionInfo":{"status":"ok","timestamp":1700550293203,"user_tz":-330,"elapsed":9344,"user":{"displayName":"5080- Priyanshi Srivastava","userId":"02728519429161914482"}}},"outputs":[],"source":["#All the required libraries and tokens are inititalised\n","import csv\n","import time\n","from google.oauth2.credentials import Credentials\n","from google_auth_oauthlib.flow import InstalledAppFlow\n","from googleapiclient.discovery import build\n","from transformers import BertTokenizer, BertForSequenceClassification\n","import torch"]},{"cell_type":"markdown","metadata":{"id":"bQSM94bdGg6A"},"source":["Mount google drive to fetch configuration and data files from the storage"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5537,"status":"ok","timestamp":1700550298725,"user":{"displayName":"5080- Priyanshi Srivastava","userId":"02728519429161914482"},"user_tz":-330},"id":"-v_zUmp6GV3q","outputId":"1633274d-1572-44ce-ffe7-0b999b279142"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"G6HrEGWJBozz"},"source":["API's and service fetch are taken here a new .json file created on personal google workshop is used."]},{"cell_type":"markdown","source":["Define the necessary variables for YouTube API access, including the scope, API service name, API version, and the path to the client secrets file stored in Google Drive."],"metadata":{"id":"hBHYiTpbQmVW"}},{"cell_type":"code","execution_count":4,"metadata":{"id":"94MWXWc9iI6-","executionInfo":{"status":"ok","timestamp":1700550298726,"user_tz":-330,"elapsed":6,"user":{"displayName":"5080- Priyanshi Srivastava","userId":"02728519429161914482"}}},"outputs":[],"source":["# Set up YouTube API credentials\n","SCOPES = [\"https://www.googleapis.com/auth/youtube.force-ssl\"]\n","API_SERVICE_NAME = \"youtube\"\n","API_VERSION = \"v3\"\n","CLIENT_SECRETS_FILE = \"/content/drive/MyDrive/Minor Project/client_secret_422487092710-ud2upt20gb8857oc2mdq5vmsn1jake0c.apps.googleusercontent.com.json\""]},{"cell_type":"markdown","metadata":{"id":"DucOb94LB26i"},"source":["BERT is initialised and token is created to use the sentiment analysis part of the transformer."]},{"cell_type":"markdown","source":["Load a pre-trained BERT model and tokenizer for sentiment analysis.\n","model_name: This constant defines the name of the hate speech detection model.\n","tokenizer: This object tokenizes the text input.\n","model: This object is used to perform hate speech detection"],"metadata":{"id":"EhqShkWPQyRu"}},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5992,"status":"ok","timestamp":1700550304714,"user":{"displayName":"5080- Priyanshi Srivastava","userId":"02728519429161914482"},"user_tz":-330},"id":"E44xWSfd_eEW","outputId":"a23765fd-f488-4e0c-8d29-1e8a66d51c18"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# Set up BERT model and tokenizer\n","model_name = 'bert-base-uncased'\n","tokenizer = BertTokenizer.from_pretrained(model_name)\n","model = BertForSequenceClassification.from_pretrained(model_name)"]},{"cell_type":"markdown","metadata":{"id":"7Ui2sCyiC3RK"},"source":["All the required functions for fetch and work are initialised and defined here."]},{"cell_type":"markdown","source":["1.Define a function to authenticate with the YouTube API using the OAuth 2.0 flow. This function returns a service object that can be used to make API requests."],"metadata":{"id":"1ZanimNaRD3B"}},{"cell_type":"markdown","source":["2. Define functions for preprocessing text and predicting hate speech using the loaded BERT model."],"metadata":{"id":"S0mYNDInSpZw"}},{"cell_type":"markdown","source":["3. Define a function to fetch live comments from a specified YouTube video using the YouTube API."],"metadata":{"id":"mYgdQcgnTSLu"}},{"cell_type":"markdown","source":["4. main execution\n","In the main part of the script:\n","Authenticate with the YouTube API.\n","Fetch live comments from a specified YouTube video.\n","Save the comments to a CSV file.\n","Perform hate speech detection on each comment and print the results."],"metadata":{"id":"Yeww2BFLT1rN"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_KL_xhiVh29M","outputId":"967eafa3-9f56-43e2-e76b-5b57a0d7f13f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=422487092710-ud2upt20gb8857oc2mdq5vmsn1jake0c.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A7587%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fyoutube.force-ssl&state=GHwCEhyMjmznYDtUZaHSDH13PXtVpL&access_type=offline\n"]}],"source":["1#TOKEN and API authentication\n","def get_authenticated_service():\n","    flow = InstalledAppFlow.from_client_secrets_file(CLIENT_SECRETS_FILE, SCOPES)\n","    credentials = flow.run_local_server(port=7587)\n","    return build(API_SERVICE_NAME, API_VERSION, credentials=credentials)\n","\n","2#Preprocessing of fetched data to give as input to the transformers\n","def preprocess_text(text):\n","    tokens = tokenizer(text, truncation=True, padding=True, return_tensors='pt')\n","    return tokens\n","\n","\n","#Sentiment analysis by the transformer to identify the hate speech comments(1) and non-hate speech comments(0)\n","def predict_hate_speech(text):\n","    inputs = preprocess_text(text)\n","    outputs = model(**inputs)\n","    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n","    prediction = torch.argmax(probabilities, dim=-1).item()\n","    return prediction, probabilities[0][1].item()\n","\n","3#Fetch call for the comments from the YouTube\n","def fetch_live_comments(youtube_service, video_id, polling_interval=10, max_comments=50):\n","    comments = []\n","    total_comments_fetched = 0\n","\n","    while total_comments_fetched < max_comments:\n","        response = youtube_service.commentThreads().list(\n","            part=\"snippet\",\n","            videoId=video_id,\n","            textFormat=\"plainText\",\n","            maxResults=100\n","        ).execute()\n","\n","        for item in response.get(\"items\", []):\n","            comment_text = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n","            comments.append(comment_text)\n","\n","        total_comments_fetched += len(response[\"items\"])\n","        print(f\"Fetched {total_comments_fetched} comments so far.\")\n","\n","        time.sleep(polling_interval)\n","\n","    return comments\n","#4\n","if __name__ == \"__main__\":\n","    youtube_service = get_authenticated_service()\n","\n","    video_id = \"dNg8OuQ7P53lP7kM\"  # Replace with the actual video ID\n","    live_comments = fetch_live_comments(youtube_service, video_id)\n","\n","    csv_filename = \"live_comments.csv\"\n","    with open(csv_filename, \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n","        writer = csv.writer(csv_file)\n","        writer.writerow([\"Comment\"])\n","        for comment in live_comments:\n","            writer.writerow([comment])\n","\n","    print(f\"Live comments saved to {csv_filename}\")\n","\n","    # Perform hate speech detection on live comments\n","    for comment in live_comments:\n","        prediction, confidence = predict_hate_speech(comment)\n","        print(f\"Comment: {comment}\")\n","        print(f\"Prediction: {'Hate speech' if prediction == 1 else 'Not hate speech'}\")\n","        print(f\"Confidence: {confidence:.2f}\\n\")\n"]}],"metadata":{"accelerator":"TPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}